{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOT+NiTQVTgBh0HUFV1PHsE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/engmoatazmostafa/Thesis/blob/main/Chexpert_CVD_diagnosis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvcQBfHSKjoF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# --- Configuration ---\n",
        "# Set the path to your CheXpert-v1.0-small directory\n",
        "data_dir = './chexpert_dataset/CheXpert-v1.0-small'\n",
        "train_csv_path = os.path.join(data_dir, 'train.csv')\n",
        "valid_csv_path = os.path.join(data_dir, 'valid.csv')\n",
        "\n",
        "# Labels of interest for CheXpert. We'll focus on 'Cardiomegaly' for CVD.\n",
        "# Other relevant labels might include 'Edema', 'Enlarged Cardiomediastinum'\n",
        "# For a comprehensive CVD diagnosis, you might combine several of these.\n",
        "CVD_LABELS = [\n",
        "    'No Finding',\n",
        "    'Enlarged Cardiomediastinum',\n",
        "    'Cardiomegaly',\n",
        "    'Lung Opacity',\n",
        "    'Lung Lesion',\n",
        "    'Edema',\n",
        "    'Consolidation',\n",
        "    'Pneumonia',\n",
        "    'Atelectasis',\n",
        "    'Pneumothorax',\n",
        "    'Pleural Effusion',\n",
        "    'Pleural Other',\n",
        "    'Fracture',\n",
        "    'Support Devices'\n",
        "]\n",
        "\n",
        "# Index of 'Cardiomegaly' in the CVD_LABELS list\n",
        "CARDIOMEGALY_IDX = CVD_LABELS.index('Cardiomegaly')\n",
        "\n",
        "# Image dimensions\n",
        "IMG_SIZE = 320\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 5\n",
        "LEARNING_RATE = 0.001\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Dataset Class ---\n",
        "class CheXpertDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None, uncertainty_strategy='U-Ones'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied on an image.\n",
        "            uncertainty_strategy (string): How to handle uncertain labels (-1).\n",
        "                                           'U-Ones': Treat -1 as 1 (positive).\n",
        "                                           'U-Zeros': Treat -1 as 0 (negative).\n",
        "                                           'U-Ignore': Ignore samples with -1 (not recommended for training).\n",
        "        \"\"\"\n",
        "        self.dataframe = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.uncertainty_strategy = uncertainty_strategy\n",
        "\n",
        "        # Map original CheXpert image paths to full paths\n",
        "        # The 'Path' column in CSV is relative to CheXpert-v1.0-small\n",
        "        self.dataframe['Path'] = self.dataframe['Path'].apply(\n",
        "            lambda x: os.path.join(self.root_dir, x)\n",
        "        )\n",
        "\n",
        "        # Select only the relevant labels for training\n",
        "        self.labels = self.dataframe[CVD_LABELS].values\n",
        "\n",
        "        # Apply uncertainty strategy\n",
        "        # CheXpert labels: 1 (positive), 0 (negative), -1 (uncertain), NaN (unmentioned)\n",
        "        # Convert NaN to 0 (unmentioned means negative for this task)\n",
        "        self.labels[np.isnan(self.labels)] = 0\n",
        "\n",
        "        if self.uncertainty_strategy == 'U-Ones':\n",
        "            self.labels[self.labels == -1] = 1\n",
        "        elif self.uncertainty_strategy == 'U-Zeros':\n",
        "            self.labels[self.labels == -1] = 0\n",
        "        elif self.uncertainty_strategy == 'U-Ignore':\n",
        "            # This strategy is more complex as it requires filtering rows.\n",
        "            # For simplicity, we'll keep it as U-Ones/U-Zeros for this example.\n",
        "            # If you truly want to ignore, you'd filter the dataframe here.\n",
        "            print(\"Warning: 'U-Ignore' strategy for uncertainty not fully implemented for training in this example. Using U-Ones/U-Zeros instead.\")\n",
        "            self.labels[self.labels == -1] = 1 # Default to U-Ones if U-Ignore is selected without full implementation.\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_path = self.dataframe.iloc[idx]['Path']\n",
        "        image = Image.open(img_path).convert('RGB') # Ensure 3 channels\n",
        "\n",
        "        labels = self.labels[idx].astype(np.float32)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.from_numpy(labels)\n",
        "\n",
        "# --- Transformations ---\n",
        "# Define transformations for training and validation data\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet stats\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# --- Load Data ---\n",
        "print(f\"Loading data from: {data_dir}\")\n",
        "train_dataset = CheXpertDataset(csv_file=train_csv_path,\n",
        "                                root_dir=data_dir,\n",
        "                                transform=train_transforms,\n",
        "                                uncertainty_strategy='U-Ones') # Common strategy for CheXpert\n",
        "val_dataset = CheXpertDataset(csv_file=valid_csv_path,\n",
        "                              root_dir=data_dir,\n",
        "                              transform=val_transforms,\n",
        "                              uncertainty_strategy='U-Ones')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "\n",
        "# --- Model Definition ---\n",
        "# Using a pre-trained DenseNet121 as a feature extractor\n",
        "class CheXpertClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CheXpertClassifier, self).__init__()\n",
        "        # Load pre-trained DenseNet121\n",
        "        self.densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
        "        # Replace the classifier layer\n",
        "        num_ftrs = self.densenet.classifier.in_features\n",
        "        self.densenet.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.densenet(x)\n",
        "\n",
        "model = CheXpertClassifier(num_classes=len(CVD_LABELS)).to(DEVICE)\n",
        "\n",
        "# --- Loss Function and Optimizer ---\n",
        "# For multi-label classification, Binary Cross-Entropy with Logits is suitable\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "\n",
        "# --- Training Function ---\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=NUM_EPOCHS):\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        start_time = time.time()\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        print(\"-\" * 10)\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f\"  Batch {i+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        print(f\"Train Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        # --- Validation ---\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs = inputs.to(DEVICE)\n",
        "                labels = labels.to(DEVICE)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Apply sigmoid to get probabilities for AUC calculation\n",
        "                preds = torch.sigmoid(outputs).cpu().numpy()\n",
        "                all_preds.append(preds)\n",
        "                all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "        val_epoch_loss = val_loss / len(val_loader.dataset)\n",
        "        print(f\"Validation Loss: {val_epoch_loss:.4f}\")\n",
        "\n",
        "        all_preds = np.vstack(all_preds)\n",
        "        all_labels = np.vstack(all_labels)\n",
        "\n",
        "        # Calculate AUC for all labels\n",
        "        try:\n",
        "            aucs = [roc_auc_score(all_labels[:, i], all_preds[:, i]) for i in range(len(CVD_LABELS))]\n",
        "            mean_auc = np.mean(aucs)\n",
        "            print(f\"Mean AUC over all {len(CVD_LABELS)} labels: {mean_auc:.4f}\")\n",
        "            print(f\"AUC for Cardiomegaly: {aucs[CARDIOMEGALY_IDX]:.4f}\")\n",
        "        except ValueError as e:\n",
        "            print(f\"Could not calculate AUC: {e}. This might happen if a class has only one label type in the batch.\")\n",
        "\n",
        "        scheduler.step(val_epoch_loss)\n",
        "\n",
        "        if val_epoch_loss < best_val_loss:\n",
        "            best_val_loss = val_epoch_loss\n",
        "            # Save the best model\n",
        "            torch.save(model.state_dict(), 'best_chexpert_model.pth')\n",
        "            print(\"Model saved as 'best_chexpert_model.pth'\")\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(f\"Epoch {epoch+1} finished in {(end_time - start_time):.2f} seconds.\")\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "    train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, NUM_EPOCHS)\n",
        "\n",
        "    print(\"\\n--- Training Complete ---\")\n",
        "    print(\"Loading the best model for final evaluation...\")\n",
        "    model.load_state_dict(torch.load('best_chexpert_model.pth'))\n",
        "    model.eval()\n",
        "\n",
        "    # --- Final Evaluation (Optional, on validation set for demonstration) ---\n",
        "    # For a real scenario, you'd use a separate test set.\n",
        "    final_preds = []\n",
        "    final_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
        "            final_preds.append(preds)\n",
        "            final_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    final_preds = np.vstack(final_preds)\n",
        "    final_labels = np.vstack(final_labels)\n",
        "\n",
        "    # Convert probabilities to binary predictions (e.g., using a threshold of 0.5)\n",
        "    # This is for metrics like precision, recall, F1-score. For AUC, probabilities are used.\n",
        "    threshold = 0.5\n",
        "    binary_preds = (final_preds > threshold).astype(int)\n",
        "\n",
        "    print(\"\\n--- Final Evaluation on Validation Set ---\")\n",
        "    for i, label_name in enumerate(CVD_LABELS):\n",
        "        print(f\"\\n--- Metrics for {label_name} ---\")\n",
        "        try:\n",
        "            auc = roc_auc_score(final_labels[:, i], final_preds[:, i])\n",
        "            print(f\"AUC: {auc:.4f}\")\n",
        "            # Classification report for precision, recall, f1-score\n",
        "            report = classification_report(final_labels[:, i], binary_preds[:, i], zero_division=0)\n",
        "            print(report)\n",
        "            # Confusion Matrix\n",
        "            cm = confusion_matrix(final_labels[:, i], binary_preds[:, i])\n",
        "            print(\"Confusion Matrix:\")\n",
        "            print(cm)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not calculate metrics for {label_name}: {e}\")\n",
        "\n",
        "    # Example: Visualize a prediction (requires a function to de-normalize and display)\n",
        "    # This part is illustrative and not fully implemented for brevity.\n",
        "    # def visualize_prediction(image_tensor, true_labels, predicted_probs, idx):\n",
        "    #     # Denormalize image\n",
        "    #     inv_normalize = transforms.Normalize(\n",
        "    #         mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "    #         std=[1/0.229, 1/0.224, 1/0.225]\n",
        "    #     )\n",
        "    #     img = inv_normalize(image_tensor).permute(1, 2, 0).cpu().numpy()\n",
        "    #     img = np.clip(img, 0, 1) # Clip values to [0,1] for display\n",
        "    #\n",
        "    #     plt.imshow(img)\n",
        "    #     plt.title(f\"True: {CVD_LABELS[np.where(true_labels == 1)[0]]}\\nPred: {CVD_LABELS[np.argmax(predicted_probs)]} ({np.max(predicted_probs):.2f})\")\n",
        "    #     plt.axis('off')\n",
        "    #     plt.show()\n",
        "\n",
        "    # You could call visualize_prediction with a sample from your validation set\n",
        "    # For example:\n",
        "    # sample_image, sample_labels = val_dataset[0]\n",
        "    # sample_image_tensor = sample_image.unsqueeze(0).to(DEVICE)\n",
        "    # with torch.no_grad():\n",
        "    #     sample_output = model(sample_image_tensor)\n",
        "    #     sample_probs = torch.sigmoid(sample_output).cpu().numpy()[0]\n",
        "    # visualize_prediction(sample_image, sample_labels, sample_probs, CARDIOMEGALY_IDX)\n"
      ]
    }
  ]
}